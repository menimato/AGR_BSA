{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f2afeca-2724-4c49-a4ae-210c53a13dcf",
   "metadata": {},
   "source": [
    "# SCRIPT 02: Create Segmentation and Generate LULC Statistics\n",
    "\n",
    "This is the second script used in the methodology. Here, the a segmentation is created with optical reduction for the year. Then, statistics are created for every polygon using LULC data from Google Dynamic world, ESA World Cover, and MapBiomas. These statistics are used to generate data regarding the agreement between these maps for the class Agriculture. Combining three LULC projects obviously do not yeld a perfect map, therefore, a posterior step must be conducted to filter the samples generated with this map as reference.\n",
    "\n",
    "In the following cells, please refer to the comments in the code for further explanations of its functioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1ee825-8aef-491c-8e8e-e8406a9e117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import time\n",
    "from rsgislib.segmentation import shepherdseg\n",
    "import rsgislib.segmentation as seg\n",
    "import rasterio as r\n",
    "import rasterio\n",
    "from rasterio.features import shapes\n",
    "import numpy as np\n",
    "import geopandas as gp\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from rasterio.mask import mask\n",
    "import shapely\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806e8e54-0879-4209-8080-581e24e72edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the function to write the log to a file\n",
    "def write_text(text, print_end='\\n'):\n",
    "    with open(\"/home/bruno.matosak/Semiarido/segmentation_log.txt\", 'a+') as f:\n",
    "        print(text, end=print_end, flush=True)\n",
    "        f.write(text+print_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1997d661-ce6f-47c8-852e-72d7347b1433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this cell, the function that makes the segmentation is defined\n",
    "def do_the_segmentation(input_img, ref_path, save_directory, save_file_name):\n",
    "    t1 = time.time()\n",
    "    # DOING THE SEGMENTATION\n",
    "    # write initial log messages\n",
    "    write_text('------------------------------------------------------------------------------------------')\n",
    "    write_text('MAKING THE SEGMENTATION WITH RSGISLIB...\\n')\n",
    "    \n",
    "    # reimport important packages\n",
    "    from rsgislib.segmentation import shepherdseg\n",
    "    import rsgislib.segmentation as seg\n",
    "    \n",
    "    # defines path of output file to save segmentation\n",
    "    out_clumps_img = os.path.join(save_directory, f'{save_file_name}.tif')\n",
    "    \n",
    "    # write log messages\n",
    "    write_text(f'Input Image: {input_img}')\n",
    "    write_text(f'Output Clumps Image: {out_clumps_img}')\n",
    "    write_text(f'Save File Name: {save_file_name}')\n",
    "    write_text(f'Reference Path: {ref_path}\\n')\n",
    "\n",
    "    # runs the segmentation for the given file\n",
    "    shepherdseg.run_shepherd_segmentation(input_img,\n",
    "                                          out_clumps_img,\n",
    "                                          out_mean_img=None,\n",
    "                                          tmp_dir=save_directory, \n",
    "                                          gdalformat='KEA',\n",
    "                                          calc_stats=False,\n",
    "                                          no_stretch=False,\n",
    "                                          no_delete=False,\n",
    "                                          num_clusters=60,\n",
    "                                          min_n_pxls=100, \n",
    "                                          dist_thres=100, \n",
    "                                          bands=None, \n",
    "                                          sampling=100, \n",
    "                                          km_max_iter=200, \n",
    "                                          process_in_mem=False, \n",
    "                                          save_process_stats=False, \n",
    "                                          img_stretch_stats='', \n",
    "                                          kmeans_centres='', \n",
    "                                          img_stats_json_file='')\n",
    "    \n",
    "    # computes elapsed time and writes it in log file\n",
    "    t2 = time.time()\n",
    "    write_text(\"Elapsed time from start: %.3f minutes.\" % ((t2-t1)/60))\n",
    "\n",
    "    # GEORREFERENCING CLUMPS OF IMAGES\n",
    "    # write some messages to log file\n",
    "    write_text('------------------------------------------------------------------------------------------')\n",
    "    write_text('GEORREFERENCING THE CLUMPS IMAGE...\\n')\n",
    "    \n",
    "    # reimport important library\n",
    "    import rasterio as r\n",
    "    \n",
    "    # defines save file geodata profile\n",
    "    profile_out = r.open(input_img).profile\n",
    "    profile_in = r.open(out_clumps_img).profile\n",
    "    img = r.open(out_clumps_img).read(1)\n",
    "    \n",
    "    # output raster path\n",
    "    out_name_geo = out_clumps_img[:-4]+'_geo.tif'\n",
    "    \n",
    "    # update profile with desired defined parameters\n",
    "    profile_out.update({'dtype': profile_in['dtype'], 'count': profile_in['count'], 'compress': 'packbits'})\n",
    "    \n",
    "    # write raster to file\n",
    "    with r.open(out_name_geo, 'w', **profile_out) as dst:\n",
    "        dst.write(img, 1)\n",
    "\n",
    "    # computes elapsed time and writes it to log file\n",
    "    t2 = time.time()\n",
    "    write_text(\"Elapsed time from start: %.3f minutes.\" % ((t2-t1)/60))\n",
    "\n",
    "    # POLYGONIZE RASTER\n",
    "    # write some log messages\n",
    "    write_text('------------------------------------------------------------------------------------------')\n",
    "    write_text('POLYGONIZING RASTER...\\n')\n",
    "    \n",
    "    # importing packages again\n",
    "    import rasterio\n",
    "    from rasterio.features import shapes\n",
    "    import numpy as np\n",
    "    import geopandas as gp\n",
    "    \n",
    "    # opens reference to copy profile\n",
    "    ref = r.open(ref_path)\n",
    "    \n",
    "    # assign None to mask\n",
    "    mask = None\n",
    "    \n",
    "    # open file and polygonize shapes\n",
    "    with rasterio.Env():\n",
    "        with rasterio.open(out_name_geo) as src:\n",
    "            image = np.asarray(src.read(1), dtype = np.int32) # first band\n",
    "            results = (\n",
    "            {'properties': {'raster_val': v}, 'geometry': s}\n",
    "            for i, (s, v) \n",
    "            in enumerate(\n",
    "                shapes(image, mask=mask, transform=src.transform)))\n",
    "    \n",
    "    # gets shapes in list\n",
    "    geoms = list(results)\n",
    "    \n",
    "    # creates geopandas GeoDataFrame with shapes and reproject it to nice CRS\n",
    "    gpd_polygonized_raster = gp.GeoDataFrame.from_features(geoms)\n",
    "    gpd_polygonized_raster = gpd_polygonized_raster.set_crs(r.open(input_img).crs)\n",
    "\n",
    "    # reimporting some packages\n",
    "    import matplotlib\n",
    "    import numpy as np\n",
    "    \n",
    "    # defining colors and plotting result to see if everything is ok\n",
    "    cmap = matplotlib.colors.ListedColormap(np.random.rand(256,3))\n",
    "    gpd_polygonized_raster.plot(cmap=cmap)\n",
    "\n",
    "    # compute elapsed time and write it to log file\n",
    "    t2 = time.time()\n",
    "    write_text(\"Elapsed time from start: %.3f minutes.\" % ((t2-t1)/60))\n",
    "\n",
    "    # ITERATING THROUGH EVERY POLYGON AND CALCULATING ITS LULC STATISTICS\n",
    "    # writing message to log file\n",
    "    write_text('------------------------------------------------------------------------------------------')\n",
    "    write_text('ITERATING THROUGH EVERY POLYGON...\\n')\n",
    "    \n",
    "    # reimporting packages\n",
    "    import rasterio as r\n",
    "    from rasterio.mask import mask\n",
    "    import shapely\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # defining loop parameters for percentage in log\n",
    "    percent_iter = 5\n",
    "    print_percent = 0\n",
    "    count = 0\n",
    "    total=len(gpd_polygonized_raster)\n",
    "    \n",
    "    # loop to analyse every polygon\n",
    "    for index, row in gpd_polygonized_raster.iterrows():\n",
    "        # gets data from LULC raster using polygon contours\n",
    "        data = np.asarray(mask(ref, [shapely.geometry.mapping(row.geometry)], crop=True, nodata=255)[0], dtype = np.float32)\n",
    "        \n",
    "        # statistics for each polygon.\n",
    "        # qt: quantity\n",
    "        # 0: not agriculture\n",
    "        # 1: agriculture\n",
    "        # GDC: Google Dynamic World\n",
    "        # ESA: ESA World Cover\n",
    "        # MB: MapBiomas\n",
    "        qt_0_GDC = np.sum(data[0]==0)\n",
    "        qt_0_ESA = np.sum(data[1]==0)\n",
    "        qt_0_MB  = np.sum(data[2]==0)\n",
    "        qt_1_GDC = np.sum(data[0]==1)\n",
    "        qt_1_ESA = np.sum(data[1]==1)\n",
    "        qt_1_MB  = np.sum(data[2]==1)\n",
    "\n",
    "        # save statistics to GeoDataFrame\n",
    "        gpd_polygonized_raster.at[index, 'qt_0_GDC'] = qt_0_GDC\n",
    "        gpd_polygonized_raster.at[index, 'qt_0_ESA'] = qt_0_ESA\n",
    "        gpd_polygonized_raster.at[index, 'qt_0_MB']  = qt_0_MB\n",
    "        gpd_polygonized_raster.at[index, 'qt_1_GDC'] = qt_1_GDC\n",
    "        gpd_polygonized_raster.at[index, 'qt_1_ESA'] = qt_1_ESA\n",
    "        gpd_polygonized_raster.at[index, 'qt_1_MB']  = qt_1_MB\n",
    "\n",
    "        # statistics for each polygon, part 2\n",
    "        # total amount of each class (agriculture, not agriculture) for the polygon\n",
    "        # GEM: considering all three sources (Google, ESA, MapBiomas)\n",
    "        # GM: considering only Google and Mapbiomas\n",
    "        qt_0_GEM = qt_0_GDC + qt_0_ESA + qt_0_MB\n",
    "        qt_1_GEM = qt_1_GDC + qt_1_ESA + qt_1_MB\n",
    "        qt_0_GM = qt_0_GDC + qt_0_MB\n",
    "        qt_1_GM = qt_1_GDC + qt_1_MB\n",
    "\n",
    "        #save statistics to GeoDataFrame, part 2\n",
    "        gpd_polygonized_raster.at[index, '0_GEM']  = qt_0_GEM\n",
    "        gpd_polygonized_raster.at[index, '1_GEM']  = qt_1_GEM\n",
    "        gpd_polygonized_raster.at[index, '0_GM']  = qt_0_GM\n",
    "        gpd_polygonized_raster.at[index, '1_GM']  = qt_1_GM\n",
    "\n",
    "        # computes which class has the majority in the polygon and also\n",
    "        # a 'reliability' metric.\n",
    "        # max: class with majority in polygon\n",
    "        # conf: reliability of the class with majority in polygon. Represents the percentage of\n",
    "        #       the majority class inside polygon.\n",
    "        if qt_0_GEM < qt_1_GEM:\n",
    "            max_GEM = 1\n",
    "            conf_GEM = 100*qt_1_GEM/(qt_0_GEM+qt_1_GEM)\n",
    "        else:\n",
    "            max_GEM = 0\n",
    "            conf_GEM = 100*qt_0_GEM/(qt_0_GEM+qt_1_GEM)\n",
    "\n",
    "        if qt_0_GM < qt_1_GM:\n",
    "            max_GM = 1\n",
    "            conf_GM = 100*qt_1_GM/(qt_0_GM+qt_1_GM)\n",
    "        else:\n",
    "            max_GM = 0\n",
    "            conf_GM = 100*qt_0_GM/(qt_0_GM+qt_1_GM)\n",
    "\n",
    "        # writes the class number with majority to GeoDataFrame\n",
    "        # also writes the reliability metric.\n",
    "        gpd_polygonized_raster.at[index, 'max_GEM']  = max_GEM\n",
    "        gpd_polygonized_raster.at[index, 'conf_GEM'] = conf_GEM\n",
    "        gpd_polygonized_raster.at[index, 'max_GM']   = max_GM\n",
    "        gpd_polygonized_raster.at[index, 'conf_GM']  = conf_GM\n",
    "\n",
    "        # adds to polygon count\n",
    "        count += 1\n",
    "\n",
    "        # writes the percentage of completion of the process\n",
    "        if 100*count/total >= print_percent:\n",
    "            write_text(f'{print_percent}...', print_end='')\n",
    "            print_percent+=percent_iter\n",
    "    \n",
    "    # writes log messages\n",
    "    write_text('')\n",
    "    write_text('Saving... ')\n",
    "    gpd_polygonized_raster.to_file(os.path.join(save_directory, f'{save_file_name}.shp'))\n",
    "    write_text('Done!')\n",
    "\n",
    "    # computes elapsed time and writes it to log file\n",
    "    t2 = time.time()\n",
    "    write_text(\"Elapsed time from start: %.3f minutes.\" % ((t2-t1)/60))\n",
    "\n",
    "    # everything is done. writing messages to log.\n",
    "    write_text('------------------------------------------------------------------------------------------')\n",
    "    write_text('ALL DONE (for now)\\n')\n",
    "\n",
    "    # computing elapsed time and write it to log\n",
    "    t2 = time.time()\n",
    "    write_text(\"Elapsed time from start: %.3f minutes.\" % ((t2-t1)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5141dd6f-8205-4fb1-9d90-38cd29c53574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this cell, there is a loop that is used to apply the segmentation to all tiles, with\n",
    "# the cell defined in the previous cell\n",
    "\n",
    "# start time\n",
    "t_start = time.time()\n",
    "    \n",
    "# load files to be segmented, creating a list with them\n",
    "files = glob.glob('/home/bruno.matosak/Semiarido/MultiInput/LULC/LULC_id*.tif')\n",
    "random.shuffle(files)\n",
    "\n",
    "# eliminating files that have already been processed from the list\n",
    "files_done = glob.glob('/home/bruno.matosak/Semiarido/MultiInput/segmentations/segmentation_id*.shp')\n",
    "for file in files_done:\n",
    "    file2 = file.replace('segmentations/segmentation', 'LULC/LULC').replace('.shp', '.tif')\n",
    "    files.remove(file2)\n",
    "\n",
    "# iterating through all the files to create the reference\n",
    "count = 1\n",
    "for file in files:\n",
    "    # gets the tile's id from the file currently being processed\n",
    "    id_ = file.split('d')[-1].split('.')[0]\n",
    "    \n",
    "    # writes messages to log file\n",
    "    write_text('==========================================================================================')\n",
    "    write_text(f'SEGMENTING ID:{id_} ({count}/{len(files)})')\n",
    "    \n",
    "    # the big boi, our segmentation\n",
    "    do_the_segmentation(input_img = file.replace('/LULC/', '/yearly_reduction/').replace('/LULC_', '/Reduction_Optical_Year_'),\n",
    "                        ref_path = file, \n",
    "                        save_directory = '/home/bruno.matosak/Semiarido/MultiInput/segmentations',\n",
    "                        save_file_name = f'segmentation_id{id_}')\n",
    "    \n",
    "    # all done!\n",
    "    write_text('\\n\\n')\n",
    "    count += 1\n",
    "\n",
    "# writes message to log file\n",
    "write_text('ALL DONE.')\n",
    "\n",
    "# computes total elapsed time and writes it to log file\n",
    "t_end = time.time()\n",
    "write_text(\"Total elapsed time: %.3f hours.\" % ((t_end-t_start)/3600))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
