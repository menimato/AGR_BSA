{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRIPT 06: Train Model with Tenfold Cross-Validation\n",
    "\n",
    "This is the sixth script in the methodology. Here, models are trained with tensorold crossvalidation in order to compare hyperparparameters and decide which are the best combination available. The models hyperparameters to be tested are defined by the user in a list of dictionaries.\n",
    "\n",
    "**ATTENTION**: with around 5000 samples, the training is very slow even with good GPUs, and since it must be repeated 10 times for each hyperparameters combination, it can take a long time run this script according to the number combinations to be tested. Be mindful of the tests you want to do.\n",
    "\n",
    "In the following cells, please refer to the comments in the code for further explanations of its functioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of hyperparameters definitions to train\n",
    "models_to_train = [{'Model ID': '01',\n",
    "                    'Learning Rate': 0.001,\n",
    "                    'Epochs': 50},\n",
    "                   \n",
    "                   {'Model ID': '02',\n",
    "                    'Learning Rate': 0.0001,\n",
    "                    'Epochs': 50},\n",
    "                   \n",
    "                   {'Model ID': '03',\n",
    "                    'Learning Rate': 0.00001,\n",
    "                    'Epochs': 50},\n",
    "                   \n",
    "                   {'Model ID': '04',\n",
    "                    'Learning Rate': 0.001,\n",
    "                    'Epochs': 100},\n",
    "                   \n",
    "                   {'Model ID': '05',\n",
    "                    'Learning Rate': 0.0001,\n",
    "                    'Epochs': 100},\n",
    "                   \n",
    "                   {'Model ID': '06',\n",
    "                    'Learning Rate': 0.00001,\n",
    "                    'Epochs': 100},\n",
    "                   \n",
    "                   {'Model ID': '07',\n",
    "                    'Learning Rate': 0.001,\n",
    "                    'Epochs': 200},\n",
    "                   \n",
    "                   {'Model ID': '08',\n",
    "                    'Learning Rate': 0.0001,\n",
    "                    'Epochs': 200},\n",
    "                   \n",
    "                   {'Model ID': '09',\n",
    "                    'Learning Rate': 0.00001,\n",
    "                    'Epochs': 200}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the samples folder and the saples id to use\n",
    "samples_folder = '/home/bruno.matosak/Semiarido/MultiInput/samples/sample_data'\n",
    "samples_id = '5K'\n",
    "\n",
    "trainings_folder = '/home/bruno.matosak/Semiarido/MultiInput/trainings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "reference = np.load(os.path.join(samples_folder, f'{samples_id}_PRO_reference.npy'))\n",
    "print('Shape Reference:', reference.shape)\n",
    "samples_s1_y = np.load(os.path.join(samples_folder, f'{samples_id}_PRO_s1_y.npy'))\n",
    "print('Shape S1 Y:', samples_s1_y.shape)\n",
    "samples_s2_y = np.load(os.path.join(samples_folder, f'{samples_id}_PRO_s2_y.npy'))\n",
    "print('Shape S2 Y:', samples_s2_y.shape)\n",
    "samples_s1_m = np.load(os.path.join(samples_folder, f'{samples_id}_PRO_s1_m.npy'))\n",
    "print('Shape S1 M:', samples_s1_m.shape)\n",
    "samples_s2_m = np.load(os.path.join(samples_folder, f'{samples_id}_PRO_s2_m.npy'))\n",
    "print('Shape S2 M:', samples_s2_m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining model\n",
    "\n",
    "# ConvLSTM block\n",
    "def ConvLSTM_block(input_tensor, num_filters, return_sequences):\n",
    "    encoder = tf.keras.layers.ConvLSTM2D(filters=num_filters, \n",
    "                                         kernel_size=(3, 3),\n",
    "                                         data_format='channels_last',\n",
    "                                         recurrent_activation='hard_sigmoid',\n",
    "                                         activation='tanh',\n",
    "                                         padding='valid',\n",
    "                                         dropout=0.3,\n",
    "                                         recurrent_dropout=0.3,\n",
    "                                         return_sequences=return_sequences)(input_tensor)\n",
    "    encoder = tf.keras.layers.BatchNormalization()(encoder)\n",
    "    encoder = tf.python.keras.layers.Activation('relu')(encoder)\n",
    "    return encoder\n",
    "\n",
    "# Simple Convolutional Block\n",
    "def simple_conv_block(input_tensor, num_filters):\n",
    "    encoder = tf.python.keras.layers.Conv2D(num_filters, (3, 3), padding='valid')(input_tensor)\n",
    "    encoder = tf.python.keras.layers.BatchNormalization()(encoder)\n",
    "    encoder = tf.python.keras.layers.Activation('relu')(encoder)\n",
    "    encoder = tf.python.keras.layers.Dropout(0.3)(encoder)\n",
    "    return encoder\n",
    "\n",
    "# Normal Convolutional Block\n",
    "def conv_block(input_tensor, num_filters):\n",
    "    encoder = tf.python.keras.layers.Conv2D(num_filters, (3, 3), padding='valid')(input_tensor)\n",
    "    encoder = tf.python.keras.layers.BatchNormalization()(encoder)\n",
    "    encoder = tf.python.keras.layers.Activation('relu')(encoder)\n",
    "    encoder = tf.python.keras.layers.Conv2D(num_filters, (3, 3), padding='valid')(encoder)\n",
    "    encoder = tf.python.keras.layers.BatchNormalization()(encoder)\n",
    "    encoder = tf.python.keras.layers.Activation('relu')(encoder)\n",
    "    encoder = tf.python.keras.layers.Dropout(0.3)(encoder)\n",
    "    return encoder\n",
    "\n",
    "# Encoder\n",
    "def encoder_block(input_tensor, num_filters):\n",
    "    encoder = conv_block(input_tensor, num_filters)\n",
    "    encoder_pool = tf.python.keras.layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n",
    "    return encoder_pool, encoder\n",
    "\n",
    "# Decoder\n",
    "def decoder_block(input_tensor, concat_tensor, num_filters, val_crop):\n",
    "    decoder = tf.python.keras.layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n",
    "    decoder = tf.python.keras.layers.concatenate([tf.keras.layers.Cropping2D(cropping=(val_crop, val_crop))(concat_tensor), decoder], axis=-1)\n",
    "    decoder = tf.python.keras.layers.BatchNormalization()(decoder)\n",
    "    decoder = tf.python.keras.layers.Activation('relu')(decoder)\n",
    "    decoder = tf.python.keras.layers.Conv2D(num_filters, (3, 3), padding='valid')(decoder)\n",
    "    decoder = tf.python.keras.layers.BatchNormalization()(decoder)\n",
    "    decoder = tf.python.keras.layers.Activation('relu')(decoder)\n",
    "    decoder = tf.python.keras.layers.Conv2D(num_filters, (3, 3), padding='valid')(decoder)\n",
    "    decoder = tf.python.keras.layers.BatchNormalization()(decoder)\n",
    "    decoder = tf.python.keras.layers.Activation('relu')(decoder)\n",
    "    decoder = tf.python.keras.layers.Dropout(0.3)(decoder)\n",
    "    return decoder\n",
    "\n",
    "# Function to create the model\n",
    "def get_model(n_times, chip_size, n_bands_s1, n_bands_s2, out_num_channels, learning_rate):\n",
    "    # organizes input\n",
    "    \n",
    "    # input 1: Sentinel-1 year reduction data\n",
    "    input1 = tf.python.keras.layers.Input(shape=[chip_size, chip_size, n_bands_s1])\n",
    "    # input 2: Sentinel-2 year reduction data\n",
    "    input2 = tf.python.keras.layers.Input(shape=[chip_size, chip_size, n_bands_s2])\n",
    "    # input 3: Sentinel-1 monthly reductions data\n",
    "    input3 = tf.python.keras.layers.Input(shape=[n_times, chip_size, chip_size, n_bands_s1])\n",
    "    # input 4: Sentinel-2 monthly reductions data\n",
    "    input4 = tf.python.keras.layers.Input(shape=[n_times, chip_size, chip_size, n_bands_s2])\n",
    "    \n",
    "    # applies the simple convolution block to yearly redcution data\n",
    "    conv_s1 = simple_conv_block(input1, n_bands_s1)\n",
    "    conv_s2 = simple_conv_block(input2, n_bands_s2)\n",
    "    \n",
    "    # applies the ConvLSTM block to monthly reductions data (they are time series)\n",
    "    conv_lstm_s1 = ConvLSTM_block(input_tensor = input3,\n",
    "                                  num_filters = n_bands_s1,\n",
    "                                  return_sequences = False)\n",
    "    conv_lstm_s2 = ConvLSTM_block(input_tensor = input4,\n",
    "                                  num_filters = n_bands_s2,\n",
    "                                  return_sequences = False)\n",
    "    \n",
    "    # concatenate the result of previous simple convolution and ConvLSTM blocks\n",
    "    concatenated = tf.python.keras.layers.concatenate([conv_s1, conv_s2, conv_lstm_s1, conv_lstm_s2], axis=-1)\n",
    "    \n",
    "    # makes the data dimension contraction and expanding paths\n",
    "    encoder0_pool, encoder0 = encoder_block(concatenated, 16) # 128\n",
    "    encoder1_pool, encoder1 = encoder_block(encoder0_pool, 32) # 64\n",
    "    encoder2_pool, encoder2 = encoder_block(encoder1_pool, 64) # 32\n",
    "    encoder3_pool, encoder3 = encoder_block(encoder2_pool, 128) # 16\n",
    "    center = conv_block(encoder3_pool, 256) # center\n",
    "    decoder3 = decoder_block(center, encoder3, 128, 4) # 64\n",
    "    decoder2 = decoder_block(decoder3, encoder2, 64, 16) # 64\n",
    "    decoder1 = decoder_block(decoder2, encoder1, 32, 40) # 128\n",
    "    decoder0 = decoder_block(decoder1, encoder0, 16, 88) # 256\n",
    "    \n",
    "    # defines the output layer\n",
    "    outputs = tf.keras.layers.Conv2D(filters=out_num_channels, kernel_size=(1,1), activation=\"softmax\", padding='same')(decoder0)\n",
    "\n",
    "    # defines the model\n",
    "    model = tf.python.keras.models.Model(inputs=[input1, input2, input3, input4], outputs=[outputs])\n",
    "\n",
    "    # compiles the model\n",
    "    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  metrics=[tf.keras.metrics.CategoricalAccuracy(), 'accuracy'])\n",
    "    \n",
    "    # returns our pretty model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this cell, functions are defined to generate grpahs showing accuracy, cateforical accuracy and\n",
    "# loss data obtained during every single training. it is used to observe how these metrics evolve during\n",
    "# the training phase and help indicate under- and over-fitting.\n",
    "\n",
    "# plot accuracy graph\n",
    "def plot_acc(h, save_folder, epochs):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    train = h.history['accuracy']\n",
    "    val = h.history['val_accuracy']\n",
    "    epochs_ = np.arange(1, epochs+1)\n",
    "    plt.plot(epochs_, train, 'g', label='Training Accuracy')\n",
    "    plt.plot(epochs_, val, 'b', label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_folder, 'accuracy.png'), facecolor='white')\n",
    "    plt.close()\n",
    "\n",
    "# plot categorical accuracy graph\n",
    "def plot_cat_acc(h, save_folder, epochs):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    train = h.history['categorical_accuracy']\n",
    "    val = h.history['val_categorical_accuracy']\n",
    "    epochs_ = np.arange(1, epochs+1)\n",
    "    plt.plot(epochs_, train, 'g', label='Training Categorical Accuracy')\n",
    "    plt.plot(epochs_, val, 'b', label='Validation Categorical Accuracy')\n",
    "    plt.title('Training and Validation Categorical Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Categorical Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_folder, 'categorical_accuracy.png'), facecolor='white')\n",
    "    plt.close()\n",
    "\n",
    "# plot loss graph\n",
    "def plot_loss(h, save_folder, epochs):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    train = h.history['loss']\n",
    "    val = h.history['val_loss']\n",
    "    epochs_ = np.arange(1, epochs+1)\n",
    "    plt.plot(epochs_, train, 'g', label='Training loss')\n",
    "    plt.plot(epochs_, val, 'b', label='Validation loss')\n",
    "    plt.title('Training and Validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_folder, 'loss.png'), facecolor='white')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to help up write messages to a log file\n",
    "def write_log_message(message):\n",
    "    with open('training_log.txt', 'a') as f:\n",
    "        f.write(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this cell, the training is done\n",
    "\n",
    "# here, the samples are divided in 10 parts to the tenfold cross-validation. each iteration is completed entirely\n",
    "# for all models before going to the next. some problem may appear and interrupt training, like hardware \n",
    "# malfunction, so all the process could be compromised, however, the samples division is made randomly according\n",
    "# to a seed number, so the samples groups are reproducible, and the training is done to the iteration numbers defined\n",
    "# by the user. this means that if some error occur and the training is interrupted, it does not need to be restarted\n",
    "# from the beginning, but from the iteration in which the error occurred. the iteration also do not need to be\n",
    "# repeated for models that the training was already completed for it, it can be started from the model that was\n",
    "# being trained when the malfunctioning happened. the script checks which model has a model file for the iteration\n",
    "# and skips it. folders with a Model.h5 file indicate that the training for it has already been completed.\n",
    "\n",
    "# to verify if training was interrupted, check the log file and see if its last modification was made an unreasonable\n",
    "# time ago, but also consider that some trainings take a really long time to complete. you can also verify the GPU and\n",
    "# CPU usage to see if the training was interrupted. furthermore, any other source of information available can be used \n",
    "# to check if traning is happening appropriately.\n",
    "\n",
    "# direct recomendations: if training is interrupted, change the values in 'iterations_to_train' to contain only the\n",
    "# ones left to be completed, so it can skip some iterations and save some time loading samples.\n",
    "\n",
    "# the iterations left to be completed\n",
    "iterations_to_train = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "# iterations counter (do not change it)\n",
    "iteration_number = 1\n",
    "\n",
    "# the samples ids, used to index all filed and reorganize them in 10 groups\n",
    "ids = np.arange(len(reference))\n",
    "\n",
    "# KFold function used to create the 10 groups of samples for tenfold cross-validation\n",
    "kfolder = KFold(10, shuffle=True, random_state=1)\n",
    "\n",
    "# loop for everyone of the 10 iterations of samples combination\n",
    "for train_ids, test_ids in kfolder.split(ids):\n",
    "    # checks if current iteration is in iterations_to_train\n",
    "    if iteration_number in iterations_to_train:\n",
    "        # write messate to log file\n",
    "        write_log_message('==============================================================\\n')\n",
    "        write_log_message(f'Iteration {iteration_number}\\n')\n",
    "\n",
    "        # separates samples and its references between train and validate for this iteration\n",
    "        # of the tenfold cross-validation process.\n",
    "        samples_train = [samples_s1_y[train_ids], samples_s2_y[train_ids], samples_s1_m[train_ids], samples_s2_m[train_ids]]\n",
    "        samples_test = [samples_s1_y[test_ids], samples_s2_y[test_ids], samples_s1_m[test_ids], samples_s2_m[test_ids]]\n",
    "        reference_train = reference[train_ids]\n",
    "        reference_test = reference[test_ids]\n",
    "\n",
    "        # loop to train this iteration of every model defined in 'models_to_train'\n",
    "        for mtt in models_to_train: # mtt: model to train\n",
    "            # defines model folder\n",
    "            model_folder = os.path.join(trainings_folder, f'model_{mtt[\"Model ID\"]}/iteration_{str(iteration_number).zfill(2)}')\n",
    "            # train if model file does not exist in folder. if model file is present, the training\n",
    "            # was already completed for this iteration.\n",
    "            if not os.path.exists(model_folder+'/Model.h5'):\n",
    "                \n",
    "                # start time for later elapsed time calculations\n",
    "                t1 = time.time()\n",
    "                \n",
    "                # write a message to log file\n",
    "                write_log_message(f'Model {mtt[\"Model ID\"]}... ')\n",
    "                \n",
    "                # defines model using previously defined function\n",
    "                model = get_model(n_times = samples_s1_m.shape[1],\n",
    "                                  chip_size = samples_s1_m.shape[2],\n",
    "                                  n_bands_s1 = samples_s1_y.shape[-1],\n",
    "                                  n_bands_s2 = samples_s2_y.shape[-1],\n",
    "                                  out_num_channels = reference.shape[-1],\n",
    "                                  learning_rate=mtt['Learning Rate'])\n",
    "\n",
    "                # make folder to acomodate training files\n",
    "                os.makedirs(model_folder, exist_ok = True)\n",
    "                # log folder\n",
    "                logdir = os.path.join(model_folder, 'fit')\n",
    "                os.makedirs(logdir, exist_ok = True)\n",
    "                # tensorboard callback\n",
    "                tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir,\n",
    "                                                                      histogram_freq = 1,\n",
    "                                                                      profile_batch = (2,10))\n",
    "                \n",
    "                # the most important line of code. here the training is done. saves training statistics\n",
    "                # per epoch to history_ for later plotting.\n",
    "                history_ = model.fit( samples_train, reference_train,\n",
    "                             validation_data=(samples_test, reference_test),\n",
    "                             batch_size=16,\n",
    "                             epochs=mtt['Epochs'],\n",
    "                             verbose=0,\n",
    "                             callbacks=[tensorboard_callback])\n",
    "\n",
    "                # plotting training statistics per epoch to files, saved in model_folder\n",
    "                plot_acc(history_, model_folder, mtt['Epochs'])\n",
    "                plot_cat_acc(history_, model_folder, mtt['Epochs'])\n",
    "                plot_loss(history_, model_folder, mtt['Epochs'])\n",
    "\n",
    "                # saving model to file\n",
    "                model.save(os.path.join(model_folder, 'Model.h5'))\n",
    "\n",
    "                # gets prediction over validation data\n",
    "                pred = tf.argmax(model.predict(samples_test, batch_size=25), -1)\n",
    "\n",
    "                # calculate confusion matrix for valication data, predicted previously, and save it to model_folder.\n",
    "                cm = confusion_matrix(np.asarray(tf.argmax(reference_test, -1)).ravel(), np.asarray(pred).ravel())\n",
    "                np.save(os.path.join(model_folder, 'test_confusion_matrix.npy'), cm)\n",
    "\n",
    "                # clear tensorflow session, so it can start a new training anew.\n",
    "                tf.keras.backend.clear_session()\n",
    "                \n",
    "                # gets elapsed time and writes it to log file.\n",
    "                t2 = time.time()\n",
    "                write_log_message('Time for training: %.3f minutes.\\n' % ((t2-t1)/60))\n",
    "\n",
    "        # break line in log file\n",
    "        write_log_message('\\n')\n",
    "    # adds to iteration number\n",
    "    iteration_number += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
