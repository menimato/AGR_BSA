{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRIPT 07: Compare Models\n",
    "\n",
    "This is the seventh script in the methodology. Here, models trained in SCRIPT 06 are compared in order to decide which one to use in the generation of a final map.\n",
    "\n",
    "In the following cells, please refer to the comments in the code for further explanations of its functioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "\n",
    "# read confusion matrices\n",
    "def read_cm(model, iteration):\n",
    "    return np.load(f'/home/bruno.matosak/Semiarido/MultiInput/trainings/model_{str(model).zfill(2)}/iteration_{str(iteration).zfill(2)}/test_confusion_matrix.npy')\n",
    "\n",
    "# calculates overall accuracy\n",
    "def overall_acc(cm):\n",
    "    return 100*np.sum(cm[np.eye(cm.shape[0], dtype=np.bool)])/np.sum(cm)\n",
    "\n",
    "# calculate F1-Score (works only for confusion matrices with 2 classes)\n",
    "def f1_score(cm, ind):\n",
    "    return 2*cm[ind,ind] / (2*cm[ind,ind] + np.sum(cm)-np.sum(cm[np.eye(len(cm), dtype=np.bool)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models to evaluate\n",
    "models = [1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "# lists to store models' metrics\n",
    "data_oa = []\n",
    "data_f1_agr = []\n",
    "data_f1_n_agr = []\n",
    "\n",
    "# looping through each model\n",
    "for model in models:\n",
    "    # lists to store metrics of a single model\n",
    "    oa = []\n",
    "    f1_agr = []\n",
    "    f1_n_agr = []\n",
    "    \n",
    "    # looping through every single iteration of the tenfold cross-validation for the model\n",
    "    for i in range(1,11,1):\n",
    "        oa.append(overall_acc(read_cm(model, i)))\n",
    "        f1_agr.append(f1_score(read_cm(model, i), 1))\n",
    "        f1_n_agr.append(f1_score(read_cm(model, i), 0))\n",
    "    \n",
    "    # append model data to models' metrics lists\n",
    "    data_oa.append(oa)\n",
    "    data_f1_agr.append(f1_agr)\n",
    "    data_f1_n_agr.append(f1_n_agr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors for plots\n",
    "colors = np.asarray(list(mcolors.CSS4_COLORS.keys()))\n",
    "ids = np.arange(len(colors))\n",
    "np.random.shuffle(ids)\n",
    "colors = colors[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot boxplots to compare models' overall accuracy\n",
    "plt.figure(figsize=(15,7))\n",
    "bplot = plt.boxplot(x=data_oa, \n",
    "                  labels=[f'Model {i}' for i in models],\n",
    "                  patch_artist=True, \n",
    "                  widths = 0.4)\n",
    "\n",
    "plt.grid()\n",
    "for patch, color in zip(bplot['boxes'], colors):\n",
    "    patch.set_facecolor('white')\n",
    "\n",
    "plt.title('Comparison Between Models - Overall Accuracy', fontweight='bold')\n",
    "plt.ylabel('Overall Accuracy (%)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot boxplots to compare models' agriculture F1-Score\n",
    "plt.figure(figsize=(15,7))\n",
    "bplot = plt.boxplot(x=data_f1_agr, \n",
    "                  labels=[f'Model {i}' for i in models],\n",
    "                  patch_artist=True, \n",
    "                  widths = 0.4)\n",
    "\n",
    "plt.grid()\n",
    "for patch, color in zip(bplot['boxes'], colors):\n",
    "    patch.set_facecolor('white')\n",
    "\n",
    "plt.title('Comparison Between Models - Agriculture\\'s F1-Score', fontweight='bold')\n",
    "plt.ylabel('F1-Score (adm.)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot boxplots to compare models' not-agriculture F1-Score\n",
    "plt.figure(figsize=(15,7))\n",
    "bplot = plt.boxplot(x=data_f1_n_agr, \n",
    "                  labels=[f'Model {i}' for i in models],\n",
    "                  patch_artist=True, \n",
    "                  widths = 0.4)\n",
    "\n",
    "plt.grid()\n",
    "for patch, color in zip(bplot['boxes'], colors):\n",
    "    patch.set_facecolor('white')\n",
    "\n",
    "plt.title('Comparison Between Models - Not-Agriculture\\'s F1-Score', fontweight='bold')\n",
    "plt.ylabel('F1-Score (adm.)')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
