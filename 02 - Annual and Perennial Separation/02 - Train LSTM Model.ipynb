{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRIPT 02 - Train LSTM Model\n",
    "\n",
    "In this script, the samples created with the previous script are used to train a linear model with LSTM layers. Each LSTM layer is followed by a Batch Normalization layer, and at the end a Dense layer connects the layers that deal with time series with the final expected result. The model is easily customizable and more about each step can be read before their respective cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell, important libraries are imported.\n",
    "\n",
    "+ `tensorflow`\n",
    "    + The library used to deal with all things deep learning in this script. With it, Keras is used as a submodule, and the model is defined, compiled, trained and stored.\n",
    "+ `numpy`\n",
    "    + Library used to open and store the training samples.\n",
    "+ `os`\n",
    "    + It is used for basic system functionalities, such as joining paths and creating folders.\n",
    "+ `pyplot`\n",
    "    + Used to plot data.\n",
    "+ `IPython.display clear_output`\n",
    "    + Used to clear the output of a cell and allow the creation of a graph for training curves that is updated on-the-fly.\n",
    "+ `sklearn.metrics confusion_matrix`\n",
    "    + Used to generate a confusion matrix, useful for model evaluation.\n",
    "+ `pandas`\n",
    "    + Necessary for using `disarray`\n",
    "+ `disarray`\n",
    "    + Used to generate many accuracy metrics that derive from a confusion matrix.\n",
    "+ `glob`\n",
    "    + USed to gather files from a folder that follow a certain name pattern. USed to get the the model with the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import disarray\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now some important defining parameters are defined.\n",
    "\n",
    "+ `model_id` (`string`)\n",
    "    + String used to define an arbitrary id for the model being trained. This will be embedded into a file path, so please only use characters allowed in file paths.\n",
    "+ `samples_id` (`string`)\n",
    "    + The id of a group of samples previously created.\n",
    "+ `path_models_folder` (`string`)\n",
    "    + A path to the folder where all models should be stored. Can be the same for models with different ids.\n",
    "+ `path_samples_folder` (`string`)\n",
    "    + A path to the folder where previously created samples are stored.\n",
    "+ `batch_size` (`int`)\n",
    "    + The amount of samples to be processed in each batch during training and prediction.\n",
    "+ `epochs` (`int`)\n",
    "    + The maximum number of epochs to be considered during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = '001'\n",
    "samples_id = '001'\n",
    "path_models_folder = '/path/to/models/folder'\n",
    "path_samples_folder = '/path/to/samples/folder'\n",
    "batch_size = 2048\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, a folder specific for the trained model is created. Its path is stored in `model_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_folder = os.path.join(path_models_folder, model_id)\n",
    "os.makedirs(model_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On of the most important code snippets of the script. Here the model is defined and compiled. Changes can be easily made here, like changing the amount of LSTM layers, or the number of units in each one of them. Loss function and optimizer can also be changed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = tf.keras.layers.Input(shape=[12,6], name='Input')\n",
    "tensor = tf.keras.layers.LSTM(24, return_sequences=True, name='LSTM_1')(input_)\n",
    "tensor = tf.keras.layers.BatchNormalization(name='BN_1')(tensor)\n",
    "tensor = tf.keras.layers.LSTM(12, return_sequences=True, name='LSTM_2')(tensor)\n",
    "tensor = tf.keras.layers.BatchNormalization(name='BN_2')(tensor)\n",
    "tensor = tf.keras.layers.LSTM(6, return_sequences=True, name='LSTM_3')(tensor)\n",
    "tensor = tf.keras.layers.BatchNormalization(name='BN_3')(tensor)\n",
    "tensor = tf.keras.layers.LSTM(4, return_sequences=True, name='LSTM_4')(tensor)\n",
    "tensor = tf.keras.layers.BatchNormalization(name='BN_4')(tensor)\n",
    "tensor = tf.keras.layers.LSTM(2, return_sequences=False, name='LSTM_5')(tensor)\n",
    "tensor = tf.keras.layers.BatchNormalization(name='BN_5')(tensor)\n",
    "tensor = tf.keras.layers.Dense(2, activation='softmax', name='Dense_1')(tensor)\n",
    "\n",
    "model = tf.python.keras.models.Model(inputs=[input_], outputs=[tensor])\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines a custom callback, in order to create a graph that shows loss and accuracy curves, updating it after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotLearning(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback to plot the learning curves of the model during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_folder):\n",
    "        self.model_folder = model_folder\n",
    "        self.epochs = 0\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.metrics = {}\n",
    "        for metric in logs:\n",
    "            self.metrics[metric] = []\n",
    "            \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.epochs = epoch\n",
    "        # Storing metrics\n",
    "        for metric in logs:\n",
    "            if metric in self.metrics:\n",
    "                self.metrics[metric].append(logs.get(metric))\n",
    "            else:\n",
    "                self.metrics[metric] = [logs.get(metric)]\n",
    "        \n",
    "        # Plotting\n",
    "        metrics = [x for x in logs if 'val' not in x]\n",
    "        \n",
    "        f, axs = plt.subplots(1, len(metrics), figsize=(15,5))\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        for i, metric in enumerate(metrics):\n",
    "            axs[i].plot(range(1, epoch + 2), \n",
    "                        self.metrics[metric], \n",
    "                        label=metric)\n",
    "            if logs['val_' + metric]:\n",
    "                axs[i].plot(range(1, epoch + 2), \n",
    "                            self.metrics['val_' + metric], \n",
    "                            label='val_' + metric)\n",
    "                \n",
    "            axs[i].legend()\n",
    "            axs[i].grid()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def on_train_end(self, logs={}):\n",
    "        # Plotting\n",
    "        metrics = [x for x in logs if 'val' not in x]\n",
    "        \n",
    "        f, axs = plt.subplots(1, len(metrics), figsize=(15,5))\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        for i, metric in enumerate(metrics):\n",
    "            axs[i].plot(range(1, self.epochs + 2), \n",
    "                        self.metrics[metric], \n",
    "                        label=metric)\n",
    "            if logs['val_' + metric]:\n",
    "                axs[i].plot(range(1, self.epochs + 2), \n",
    "                            self.metrics['val_' + metric], \n",
    "                            label='val_' + metric)\n",
    "                \n",
    "            axs[i].legend()\n",
    "            axs[i].grid()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.model_folder, \"training_metrics.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code loads training samples and convert the format found in the way reference was stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_values = np.load(os.path.join(path_to_samples_folder, f'{samples_id}_scaled.npy'))\n",
    "samples_reference = tf.keras.utils.to_categorical(np.load(os.path.join(path_to_samples_folder, f'{samples_id}_reference.npy'))-1)\n",
    "\n",
    "samples_values.shape, samples_reference.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell the model is fitted to the training samples. Notice the callback ModelCheckpoint, it saves the model after each epoch if it showed the greatest accuracy for validation data until this point, therefore, the latest model saved in the model folder will always be the one with highest accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(os.path.join(model_folder,'model-{epoch:03d}.h5'), save_best_only=True)\n",
    "\n",
    "history_ = model.fit(samples_values[:int(len(samples_values)*0.8)], samples_reference[:int(len(samples_values)*0.8)],\n",
    "                     validation_data=(samples_values[int(len(samples_values)*0.8):],\n",
    "                                      samples_reference[int(len(samples_values)*0.8):]),\n",
    "                     batch_size=batch_size,\n",
    "                     epochs=epochs,\n",
    "                     verbose=0,\n",
    "                     callbacks=[checkpoint, PlotLearning(model_folder)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code loads the model saved with the highest accuracy, because the model after the last training epoch not necessarily is the most accurate. This step helps avoiding overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_paths = glob.glob(os.path.join(model_folder,'*.h5'))\n",
    "models_paths.sort()\n",
    "print('Highest Accuracy Model:', models_paths[-1])\n",
    "model = tf.keras.models.load_model(models_paths[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, an evaluation of the model is made. Also, a confusion matrix considering for the validation samples is also generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x=samples_values[int(len(samples_values)*0.8):], \n",
    "               y=samples_reference[int(len(samples_values)*0.8):],\n",
    "               batch_size=batch_size)\n",
    "\n",
    "pred = np.argmax(model.predict(samples_values[int(len(samples_values)*0.8):], batch_size=batch_size), axis=-1)\n",
    "\n",
    "cm = confusion_matrix(y_pred = pred, y_true=np.argmax(samples_reference[int(len(samples_values)*0.8):], axis=-1))\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves information about model layers to a .txt file, for later reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(model_folder,'model_layers.txt'), 'w+') as file:\n",
    "    model.summary(print_fn=lambda x: file.write(x + '\\n'))\n",
    "    file.write(str(model.evaluate(x=samples_values[int(len(samples_values)*0.8):],\n",
    "               y=samples_reference[int(len(samples_values)*0.8):],\n",
    "               batch_size=batch_size)))\n",
    "    file.write(str(cm))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
